{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90279,"databundleVersionId":10477255,"sourceType":"competition"},{"sourceId":199945,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":170561,"modelId":192875}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install efficientnet_pytorch\n!pip install torch_optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:48:42.590926Z","iopub.execute_input":"2024-12-16T08:48:42.593219Z","iopub.status.idle":"2024-12-16T08:49:02.724533Z","shell.execute_reply.started":"2024-12-16T08:48:42.593159Z","shell.execute_reply":"2024-12-16T08:49:02.723434Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import io\nimport random\nimport os\nimport math\nimport timm\nfrom PIL import Image\nfrom tqdm import tqdm\nimport gc\nimport pandas as pd\nimport multiprocessing\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom efficientnet_pytorch import EfficientNet\nfrom torchvision.models import efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\n\nfrom torch.cuda.amp import GradScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:49:10.403383Z","iopub.execute_input":"2024-12-16T08:49:10.404110Z","iopub.status.idle":"2024-12-16T08:49:17.506356Z","shell.execute_reply.started":"2024-12-16T08:49:10.404073Z","shell.execute_reply":"2024-12-16T08:49:17.505405Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Constants\nBATCH_SIZE = 16\nGRADIENT_ACCUMULATION_STEPS = 2\nNUM_WORKERS = 2\nIMAGE_SIZE = 320 \nPIN_MEMORY = True \nPATIENCE = 5\nN_FOLDS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:50:19.976611Z","iopub.execute_input":"2024-12-16T08:50:19.977283Z","iopub.status.idle":"2024-12-16T08:50:19.981243Z","shell.execute_reply.started":"2024-12-16T08:50:19.977243Z","shell.execute_reply":"2024-12-16T08:50:19.980300Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def calculate_dataset_stats(dataframe, image_dir):\n    \"\"\"Calculate mean and std of the dataset\"\"\"\n    print(\"Calculating dataset mean and std...\")\n    \n    # Basic transforms just for stats calculation\n    basic_transforms = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor()\n    ])\n    \n    class StatsDataset(torch.utils.data.Dataset):\n        def __init__(self, df, img_dir, transform):\n            self.df = df\n            self.img_dir = img_dir\n            self.transform = transform\n        \n        def __len__(self):\n            return len(self.df)\n        \n        def __getitem__(self, idx):\n            img_path = os.path.join(self.img_dir, self.df.iloc[idx].filename)\n            image = Image.open(img_path).convert('RGB')\n            return self.transform(image)\n    \n    # Create dataset and loader for stats calculation\n    stats_dataset = StatsDataset(dataframe, image_dir, basic_transforms)\n    stats_loader = DataLoader(\n        stats_dataset,\n        batch_size=32,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    means = []\n    stds = []\n    \n    # Calculate mean and std\n    for batch in tqdm(stats_loader, desc=\"Calculating dataset statistics\"):\n        means.append(batch.mean((0,2,3)))\n        stds.append(batch.std((0,2,3)))\n    \n    dataset_mean = torch.stack(means).mean(0)\n    dataset_std = torch.stack(stds).mean(0)\n    \n    print(f\"Dataset mean: {dataset_mean}\")\n    print(f\"Dataset std: {dataset_std}\")\n    \n    return dataset_mean, dataset_std\n\nclass ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, image_dir: str, mode: str, mean=None, std=None):\n        self.df = dataframe\n        self.mode = mode\n        self.image_dir = image_dir\n        \n        # Use calculated stats or ImageNet stats as fallback\n        self.mean = mean if mean is not None else [0.485, 0.456, 0.406]\n        self.std = std if std is not None else [0.229, 0.224, 0.225]\n        \n        if self.mode == 'train':\n            self.transforms = transforms.Compose([\n                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomVerticalFlip(p=0.3),\n                transforms.RandomRotation(15),\n                transforms.ColorJitter(\n                    brightness=0.2, \n                    contrast=0.2, \n                    saturation=0.2, \n                    hue=0.1\n                ),\n                transforms.RandomAffine(\n                    degrees=10, \n                    translate=(0.1, 0.1), \n                    scale=(0.9, 1.1)\n                ),\n                transforms.RandomGrayscale(p=0.1),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=self.mean, std=self.std)\n            ])\n        else:\n            self.transforms = transforms.Compose([\n                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=self.mean, std=self.std)\n            ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index: int):\n        try:\n            row = self.df.iloc[index]\n            image_path = os.path.join(self.image_dir, row.filename)\n            \n            try:\n                image = Image.open(image_path).convert('RGB')\n                image = self.transforms(image)\n            except Exception as e:\n                print(f\"Error loading image {image_path}: {str(e)}\")\n                raise e\n\n            if self.mode == 'test':\n                return {\n                    'image': image,\n                    'filename': row.filename\n                }\n            else:\n                return {\n                    'image': image,\n                    'target': row.city_id,\n                    'filename': row.filename\n                }\n        except Exception as e:\n            print(f\"Error in __getitem__ at index {index}: {str(e)}\")\n            raise e\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:50:20.800016Z","iopub.execute_input":"2024-12-16T08:50:20.800795Z","iopub.status.idle":"2024-12-16T08:50:20.814355Z","shell.execute_reply.started":"2024-12-16T08:50:20.800746Z","shell.execute_reply":"2024-12-16T08:50:20.813453Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_data(train_df, test_df, train_dir, test_dir, fold=0, mean=None, std=None):\n    print(\"Preparing data loaders...\")\n    \n    label_encoder = LabelEncoder()\n    train_df['city_id'] = label_encoder.fit_transform(train_df['city'])\n    num_classes = len(label_encoder.classes_)\n    \n    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n    valid_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n    \n    train_dataset = ImageDataset(train_data, train_dir, mode='train', mean=mean, std=std)\n    valid_dataset = ImageDataset(valid_data, train_dir, mode='valid', mean=mean, std=std)\n    test_dataset = ImageDataset(test_df, test_dir, mode='test', mean=mean, std=std)\n\n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Validation dataset size: {len(valid_dataset)}\")\n    print(f\"Test dataset size: {len(test_dataset)}\")\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        drop_last=True\n    )\n    \n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    return train_loader, valid_loader, test_loader, label_encoder, num_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:54:44.338832Z","iopub.execute_input":"2024-12-16T08:54:44.339329Z","iopub.status.idle":"2024-12-16T08:54:44.347690Z","shell.execute_reply.started":"2024-12-16T08:54:44.339281Z","shell.execute_reply":"2024-12-16T08:54:44.346642Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import timm\nfrom timm import create_model\n\nclass EfficientNetB6Head(nn.Module):\n    def __init__(self, num_classes, dropout_rate=0.5):\n        super().__init__()\n        self.encoder = create_model(\n            'tf_efficientnet_b6_ns',\n            pretrained=True,\n            num_classes=0\n        )\n        \n        # Freeze some early layers\n        for name, param in list(self.encoder.named_parameters())[:100]:\n            param.requires_grad = False\n            \n        n_features = self.encoder.num_features\n        \n        # More gradual reduction in dimensions\n        # Added LayerNorm for better regularization\n        self.head = nn.Sequential(\n            nn.Linear(n_features, 1536),\n            nn.LayerNorm(1536),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(1536, 768),\n            nn.LayerNorm(768),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(768, 384),\n            nn.LayerNorm(384),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(384, num_classes)\n        )\n        \n    def forward(self, x):\n        features = self.encoder(x)\n        return self.head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:50:21.885483Z","iopub.execute_input":"2024-12-16T08:50:21.885845Z","iopub.status.idle":"2024-12-16T08:50:21.892850Z","shell.execute_reply.started":"2024-12-16T08:50:21.885812Z","shell.execute_reply":"2024-12-16T08:50:21.891893Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import timm\nfrom timm import create_model\nimport torch.nn as nn\n\nclass EfficientNetV2SmallHead(nn.Module):\n    def __init__(self, num_classes, dropout_rate=0.5):\n        super().__init__()\n        self.encoder = create_model(\n            'tf_efficientnetv2_s',  # Changed to EfficientNetV2 Small\n            pretrained=True,\n            num_classes=0\n        )\n        \n        # Freeze early layers (adjusted for EfficientNetV2 Small)\n        for name, param in list(self.encoder.named_parameters())[:70]:  # Reduced from 100 due to different architecture\n            param.requires_grad = False\n            \n        n_features = self.encoder.num_features  # EfficientNetV2 Small has different feature dimensions\n        \n        # Adjusted head dimensions for EfficientNetV2 Small\n        self.head = nn.Sequential(\n            nn.Linear(n_features, 1024),  # Reduced from 1536 due to smaller backbone\n            nn.LayerNorm(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(1024, 512),  # Reduced from 768\n            nn.LayerNorm(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(512, 256),  # Reduced from 384\n            nn.LayerNorm(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, x):\n        features = self.encoder(x)\n        return self.head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:50:22.229998Z","iopub.execute_input":"2024-12-16T08:50:22.230505Z","iopub.status.idle":"2024-12-16T08:50:22.237666Z","shell.execute_reply.started":"2024-12-16T08:50:22.230460Z","shell.execute_reply":"2024-12-16T08:50:22.236815Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def calculate_macro_f1(preds, targets, num_classes):\n\n    # Tensor'ları numpy array'e çevir\n    if torch.is_tensor(preds):\n        preds = preds.cpu().numpy()\n    if torch.is_tensor(targets):\n        targets = targets.cpu().numpy()\n    \n    # Her şehir için F1 skorunu hesapla\n    city_f1_scores = []\n    \n    for city_idx in range(num_classes):\n        # True Positives: Doğru tahmin edilen şehir sayısı\n        tp = np.sum((preds == city_idx) & (targets == city_idx))\n        \n        # False Positives: Yanlış şehir olarak tahmin edilenler\n        fp = np.sum((preds == city_idx) & (targets != city_idx))\n        \n        # False Negatives: Kaçırılan şehir tahminleri\n        fn = np.sum((preds != city_idx) & (targets == city_idx))\n        \n        # Precision hesapla\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        \n        # Recall hesapla\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        \n        # F1 skoru hesapla\n        if precision + recall > 0:\n            f1 = 2 * (precision * recall) / (precision + recall)\n        else:\n            f1 = 0.0\n        \n        city_f1_scores.append(f1)\n    \n    # Macro F1: Tüm şehirlerin F1 skorlarının ortalaması\n    macro_f1 = np.mean(city_f1_scores)\n    \n    return float(macro_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:50:22.728610Z","iopub.execute_input":"2024-12-16T08:50:22.729417Z","iopub.status.idle":"2024-12-16T08:50:22.735928Z","shell.execute_reply.started":"2024-12-16T08:50:22.729381Z","shell.execute_reply":"2024-12-16T08:50:22.735100Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\nclass EnsemblePredictor:\n    def __init__(self, models, weights=None, device='cuda'):\n        \"\"\"\n        Args:\n            models (list): Liste halinde modeller [EfficientNetB6Head, EfficientNetV2SmallHead]\n            weights (list, optional): Her modelin ağırlığı. Default olarak eşit ağırlık.\n            device (str): Kullanılacak cihaz ('cuda' veya 'cpu')\n        \"\"\"\n        self.models = models\n        self.device = device\n        \n        if weights is None:\n            self.weights = [1/len(models)] * len(models)\n        else:\n            # Ağırlıkları normalize et\n            total = sum(weights)\n            self.weights = [w/total for w in weights]\n            \n        # Modelleri eval moduna al ve GPU'ya taşı\n        for model in self.models:\n            model.eval()\n            model.to(self.device)\n    \n    @torch.no_grad()\n    def predict_batch(self, batch):\n        \"\"\"\n        Batch için ensemble tahmin yapar\n        \n        Args:\n            batch (torch.Tensor): Model inputu olarak görüntü batch'i\n            \n        Returns:\n            predictions (torch.Tensor): Tahmin edilen sınıf indeksleri\n            ensemble_probas (torch.Tensor): Her sınıf için ensemble olasılıkları\n        \"\"\"\n        # Her model için olasılıkları topla\n        probas = []\n        \n        for model, weight in zip(self.models, self.weights):\n            output = model(batch)\n            proba = torch.softmax(output, dim=1) * weight\n            probas.append(proba)\n        \n        # Ağırlıklı ortalama al\n        ensemble_probas = sum(probas)\n        \n        # En yüksek olasılıklı sınıfı seç\n        _, predictions = torch.max(ensemble_probas, 1)\n        \n        return predictions, ensemble_probas\n    \n    @torch.no_grad()\n    def validate(self, valid_loader, criterion):\n        \"\"\"\n        Validation seti üzerinde ensemble modelini değerlendir\n        \n        Args:\n            valid_loader (DataLoader): Validation data loader\n            criterion: Loss function\n            \n        Returns:\n            avg_loss (float): Ortalama validation loss\n            avg_f1 (float): Ortalama validation F1 score\n        \"\"\"\n        running_loss = 0.0\n        running_f1 = 0.0\n        steps = 0\n        \n        for data in tqdm(valid_loader, desc='Validating Ensemble'):\n            images = data['image'].cuda(non_blocking=True)\n            targets = data['target'].cuda(non_blocking=True)\n            \n            # Ensemble tahminleri al\n            preds, probas = self.predict_batch(images)\n            \n            # Loss hesapla\n            loss = criterion(probas, targets)\n            \n            # F1 hesapla\n            f1 = calculate_macro_f1(preds, targets, probas.size(1))\n            \n            running_loss += loss.item()\n            running_f1 += f1\n            steps += 1\n            \n            del images, probas, loss\n            torch.cuda.empty_cache()\n        \n        return running_loss / steps, running_f1 / steps\n    \n    def generate_submission(self, test_loader, label_encoder):\n        \"\"\"\n        Test seti için submission dosyası oluştur\n        \n        Args:\n            test_loader (DataLoader): Test data loader\n            label_encoder: Label encoder object\n            \n        Returns:\n            submission (pd.DataFrame): Submission dataframe\n        \"\"\"\n        predictions = []\n        filenames = []\n        probabilities = []\n        \n        for data in tqdm(test_loader, desc='Generating Predictions'):\n            images = data['image'].cuda(non_blocking=True)\n            \n            # Ensemble tahminleri al\n            preds, probas = self.predict_batch(images)\n            \n            predictions.extend(label_encoder.inverse_transform(preds.cpu().numpy()))\n            filenames.extend(data['filename'])\n            probabilities.extend(probas.cpu().numpy())\n            \n            del images, preds, probas\n            torch.cuda.empty_cache()\n        \n        # Submission dataframe oluştur\n        submission = pd.DataFrame({\n            'filename': filenames,\n            'city': predictions\n        })\n        \n        # Tahmin olasılıklarını da kaydet\n        probabilities = np.array(probabilities)\n        for i, city in enumerate(label_encoder.classes_):\n            submission[f'{city}_probability'] = probabilities[:, i]\n        \n        return submission\n        \n    def get_model_predictions(self, batch):\n        \"\"\"\n        Her modelin ayrı ayrı tahminlerini döndür (debug için)\n        \n        Args:\n            batch (torch.Tensor): Model inputu olarak görüntü batch'i\n            \n        Returns:\n            model_predictions (list): Her modelin tahminleri\n            model_probas (list): Her modelin olasılık dağılımları\n        \"\"\"\n        model_predictions = []\n        model_probas = []\n        \n        for model in self.models:\n            output = model(batch)\n            proba = torch.softmax(output, dim=1)\n            _, preds = torch.max(proba, 1)\n            \n            model_predictions.append(preds)\n            model_probas.append(proba)\n            \n        return model_predictions, model_probas\n\ndef load_models(efficientnetb6_path, efficientnetv2_path, num_classes):\n    \"\"\"\n    Eğitilmiş model checkpointlerini yükle\n    \"\"\"\n    # EfficientNetB6 modelini yükle\n    efficientnetb6 = EfficientNetB6Head(num_classes=num_classes)\n    efficientnetb6_state = torch.load(efficientnetb6_path, map_location='cuda')\n    efficientnetb6.load_state_dict(efficientnetb6_state['model_state_dict'])\n    \n    # EfficientNetV2 modelini yükle\n    efficientnetv2 = EfficientNetV2SmallHead(num_classes=num_classes)\n    efficientnetv2_state = torch.load(efficientnetv2_path, map_location='cuda')\n    efficientnetv2.load_state_dict(efficientnetv2_state['model_state_dict'])\n    \n    return [efficientnetb6, efficientnetv2]\n\n@torch.no_grad()\ndef validate_ensemble(valid_loader, ensemble_predictor, criterion):\n    \"\"\"\n    Validation seti üzerinde ensemble modelini değerlendir\n    \"\"\"\n    running_loss = 0.0\n    running_f1 = 0.0\n    steps = 0\n    \n    for data in tqdm(valid_loader, desc='Validating Ensemble'):\n        images = data['image'].cuda(non_blocking=True)\n        targets = data['target'].cuda(non_blocking=True)\n        \n        # Ensemble tahminleri al\n        preds, probas = ensemble_predictor.predict_batch(images)\n        \n        # Loss hesapla\n        loss = criterion(probas, targets)\n        \n        # F1 hesapla\n        f1 = calculate_macro_f1(preds, targets, probas.size(1))\n        \n        running_loss += loss.item()\n        running_f1 += f1\n        steps += 1\n        \n        del images, probas, loss\n        torch.cuda.empty_cache()\n    \n    return running_loss / steps, running_f1 / steps\n\ndef generate_ensemble_submission(test_loader, ensemble_predictor, label_encoder):\n    predictions = []\n    filenames = []\n    \n    for data in tqdm(test_loader, desc='Generating Predictions'):\n        images = data['image'].cuda(non_blocking=True)\n        \n        # Ensemble tahminleri al\n        preds, _ = ensemble_predictor.predict_batch(images)  # * yerine _ kullanılmalı\n        \n        predictions.extend(label_encoder.inverse_transform(preds.cpu().numpy()))\n        filenames.extend(data['filename'])\n        \n        del images, preds\n        torch.cuda.empty_cache()\n    \n    submission = pd.DataFrame({\n        'filename': filenames,\n        'city': predictions\n    })\n    \n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:50:23.328964Z","iopub.execute_input":"2024-12-16T08:50:23.329326Z","iopub.status.idle":"2024-12-16T08:50:23.349213Z","shell.execute_reply.started":"2024-12-16T08:50:23.329292Z","shell.execute_reply":"2024-12-16T08:50:23.348328Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def prepare_data(train_df):\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n    train_df['fold'] = -1\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['city'])):\n        train_df.loc[val_idx, 'fold'] = fold\n    \n    return train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:52:54.961629Z","iopub.execute_input":"2024-12-16T08:52:54.962323Z","iopub.status.idle":"2024-12-16T08:52:54.967092Z","shell.execute_reply.started":"2024-12-16T08:52:54.962288Z","shell.execute_reply":"2024-12-16T08:52:54.966041Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Kaggle paths\n    KAGGLE_INPUT = '/kaggle/input/datathon-ai-qualification-round'\n    \n    # Load data\n    train = pd.read_csv(f'{KAGGLE_INPUT}/train_data.csv')\n    test = pd.read_csv(f'{KAGGLE_INPUT}/test.csv')\n    \n    # Set correct image directories\n    train_dir = f'{KAGGLE_INPUT}/train/train'\n    test_dir = f'{KAGGLE_INPUT}/test/test'\n    \n    # Print dataset info\n    print(\"Dataset Information:\")\n    print(f\"Training samples: {len(train)}\")\n    print(f\"Test samples: {len(test)}\")\n    print(\"\\nSample training data:\")\n    print(train.head())\n    print(\"\\nSample test data:\")\n    print(test.head())\n    \n    # Verify paths exist\n    for path in [train_dir, test_dir]:\n        if not os.path.exists(path):\n            raise ValueError(f\"Path does not exist: {path}\")\n    \n    # First prepare folds\n    train = prepare_data(train)\n    \n    # Calculate dataset statistics ONLY on training fold\n    training_fold = 0\n    train_fold_data = train[train['fold'] != training_fold].reset_index(drop=True)\n    \n    print(f\"Calculating statistics using {len(train_fold_data)} training samples...\")\n    train_mean, train_std = calculate_dataset_stats(train_fold_data, train_dir)\n    \n    # Enable memory optimizations\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    try:\n        # Set device\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {device}\")\n        \n        # Train on fold 0\n        train_loader, valid_loader, test_loader, label_encoder, num_classes = load_data(\n            train, test, train_dir, test_dir, fold=training_fold,\n            mean=train_mean.tolist(),\n            std=train_std.tolist()\n        )\n        \n        # Model paths\n        EFFICIENTNETB6_PATH = '/kaggle/input/efficientnet/pytorch/default/1/reg_efficientnetb6_imgsize320_simulatedbatchsize32.pth'\n        EFFICIENTNETV2_PATH = '/kaggle/input/efficientnet/pytorch/default/1/reg_efficientnetv2_imgsize320_simulatedbatchsize32.pth'\n        \n        print(\"Loading pretrained models...\")\n        # Load models\n        models = load_models(EFFICIENTNETB6_PATH, EFFICIENTNETV2_PATH, num_classes=num_classes)\n        \n        # Model weights based on validation F1 scores\n        weights = [0.92928, 0.92190]\n        \n        print(\"Creating ensemble predictor...\")\n        # Create ensemble predictor\n        ensemble_predictor = EnsemblePredictor(\n            models=models,\n            weights=weights,\n            device=device\n        )\n        \n        print(\"Generating predictions...\")\n        # Generate predictions with probabilities\n        submission = ensemble_predictor.generate_submission(test_loader, label_encoder)\n        \n        # Save submissions\n        submission_path = '/kaggle/working/submission.csv'\n        detailed_submission_path = '/kaggle/working/detailed_submission.csv'\n        \n        # Save basic submission\n        final_submission = submission[['filename', 'city']]\n        final_submission.to_csv(submission_path, index=False)\n        \n        # Save detailed submission with probabilities\n        submission.to_csv(detailed_submission_path, index=False)\n        \n        print(f\"Submissions saved to: {submission_path} and {detailed_submission_path}\")\n        \n        # Verify submission format\n        print(\"\\nVerifying submission format...\")\n        if set(final_submission.columns) != {'filename', 'city'}:\n            print(\"Warning: Submission columns do not match required format!\")\n        if not all(final_submission['city'].isin(['Istanbul', 'Ankara', 'Izmir'])):\n            print(\"Warning: Submission contains invalid city names!\")\n        \n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        raise\n    \n    finally:\n        # Clean up\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T08:54:54.843233Z","iopub.execute_input":"2024-12-16T08:54:54.843944Z","iopub.status.idle":"2024-12-16T08:56:05.707581Z","shell.execute_reply.started":"2024-12-16T08:54:54.843907Z","shell.execute_reply":"2024-12-16T08:56:05.706555Z"}},"outputs":[{"name":"stdout","text":"Dataset Information:\nTraining samples: 7000\nTest samples: 2000\n\nSample training data:\n          filename      city\n0  image_10000.jpg  Istanbul\n1  image_10001.jpg  Istanbul\n2  image_10002.jpg    Ankara\n3  image_10003.jpg     Izmir\n4  image_10004.jpg    Ankara\n\nSample test data:\n          filename  city\n0  image_17000.jpg   NaN\n1  image_17001.jpg   NaN\n2  image_17002.jpg   NaN\n3  image_17003.jpg   NaN\n4  image_17004.jpg   NaN\nCalculating statistics using 5600 training samples...\nCalculating dataset mean and std...\n","output_type":"stream"},{"name":"stderr","text":"Calculating dataset statistics: 100%|██████████| 175/175 [00:33<00:00,  5.27it/s]\n/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b6_ns to current tf_efficientnet_b6.ns_jft_in1k.\n  model = create_fn(\n","output_type":"stream"},{"name":"stdout","text":"Dataset mean: tensor([0.5070, 0.5327, 0.5378])\nDataset std: tensor([0.2318, 0.2396, 0.2917])\nUsing device: cuda\nPreparing data loaders...\nTrain dataset size: 5600\nValidation dataset size: 1400\nTest dataset size: 2000\nLoading pretrained models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/173M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319ed082f83047cca8c049bd1c10c96b"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/1295311663.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  efficientnetb6_state = torch.load(efficientnetb6_path, map_location='cuda')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf4a8a286cd4ee2b12610f37fdb6ec3"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/1295311663.py:174: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  efficientnetv2_state = torch.load(efficientnetv2_path, map_location='cuda')\n","output_type":"stream"},{"name":"stdout","text":"Creating ensemble predictor...\nGenerating predictions...\n","output_type":"stream"},{"name":"stderr","text":"Generating Predictions: 100%|██████████| 125/125 [00:27<00:00,  4.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Submissions saved to: /kaggle/working/submission.csv and /kaggle/working/detailed_submission.csv\n\nVerifying submission format...\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}