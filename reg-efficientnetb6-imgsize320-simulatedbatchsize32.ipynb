{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90279,"databundleVersionId":10477255,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install efficientnet_pytorch\n!pip install torch_optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:01.936065Z","iopub.execute_input":"2024-12-16T02:08:01.936861Z","iopub.status.idle":"2024-12-16T02:08:17.962449Z","shell.execute_reply.started":"2024-12-16T02:08:01.936819Z","shell.execute_reply":"2024-12-16T02:08:17.961285Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import io\nimport random\nimport os\nimport math\nimport timm\nfrom PIL import Image\nfrom tqdm import tqdm\nimport gc\nimport pandas as pd\nimport multiprocessing\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom efficientnet_pytorch import EfficientNet\nfrom torchvision.models import efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\n\nfrom torch.cuda.amp import GradScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:17.964972Z","iopub.execute_input":"2024-12-16T02:08:17.965273Z","iopub.status.idle":"2024-12-16T02:08:17.971827Z","shell.execute_reply.started":"2024-12-16T02:08:17.965243Z","shell.execute_reply":"2024-12-16T02:08:17.971065Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Constants\nBATCH_SIZE = 16\nGRADIENT_ACCUMULATION_STEPS = 2\nNUM_WORKERS = 2\nIMAGE_SIZE = 320 \nPIN_MEMORY = True \nPATIENCE = 5\nN_FOLDS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:17.972996Z","iopub.execute_input":"2024-12-16T02:08:17.973248Z","iopub.status.idle":"2024-12-16T02:08:17.985681Z","shell.execute_reply.started":"2024-12-16T02:08:17.973224Z","shell.execute_reply":"2024-12-16T02:08:17.985019Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def calculate_dataset_stats(dataframe, image_dir):\n    \"\"\"Calculate mean and std of the dataset\"\"\"\n    print(\"Calculating dataset mean and std...\")\n    \n    # Basic transforms just for stats calculation\n    basic_transforms = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor()\n    ])\n    \n    class StatsDataset(torch.utils.data.Dataset):\n        def __init__(self, df, img_dir, transform):\n            self.df = df\n            self.img_dir = img_dir\n            self.transform = transform\n        \n        def __len__(self):\n            return len(self.df)\n        \n        def __getitem__(self, idx):\n            img_path = os.path.join(self.img_dir, self.df.iloc[idx].filename)\n            image = Image.open(img_path).convert('RGB')\n            return self.transform(image)\n    \n    # Create dataset and loader for stats calculation\n    stats_dataset = StatsDataset(dataframe, image_dir, basic_transforms)\n    stats_loader = DataLoader(\n        stats_dataset,\n        batch_size=32,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    means = []\n    stds = []\n    \n    # Calculate mean and std\n    for batch in tqdm(stats_loader, desc=\"Calculating dataset statistics\"):\n        means.append(batch.mean((0,2,3)))\n        stds.append(batch.std((0,2,3)))\n    \n    dataset_mean = torch.stack(means).mean(0)\n    dataset_std = torch.stack(stds).mean(0)\n    \n    print(f\"Dataset mean: {dataset_mean}\")\n    print(f\"Dataset std: {dataset_std}\")\n    \n    return dataset_mean, dataset_std\n\nclass ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, image_dir: str, mode: str, mean=None, std=None):\n        self.df = dataframe\n        self.mode = mode\n        self.image_dir = image_dir\n        \n        # Use calculated stats or ImageNet stats as fallback\n        self.mean = mean if mean is not None else [0.485, 0.456, 0.406]\n        self.std = std if std is not None else [0.229, 0.224, 0.225]\n        \n        if self.mode == 'train':\n            self.transforms = transforms.Compose([\n                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomVerticalFlip(p=0.3),\n                transforms.RandomRotation(15),\n                transforms.ColorJitter(\n                    brightness=0.2, \n                    contrast=0.2, \n                    saturation=0.2, \n                    hue=0.1\n                ),\n                transforms.RandomAffine(\n                    degrees=10, \n                    translate=(0.1, 0.1), \n                    scale=(0.9, 1.1)\n                ),\n                transforms.RandomGrayscale(p=0.1),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=self.mean, std=self.std)\n            ])\n        else:\n            self.transforms = transforms.Compose([\n                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=self.mean, std=self.std)\n            ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index: int):\n        try:\n            row = self.df.iloc[index]\n            image_path = os.path.join(self.image_dir, row.filename)\n            \n            try:\n                image = Image.open(image_path).convert('RGB')\n                image = self.transforms(image)\n            except Exception as e:\n                print(f\"Error loading image {image_path}: {str(e)}\")\n                raise e\n\n            if self.mode == 'test':\n                return {\n                    'image': image,\n                    'filename': row.filename\n                }\n            else:\n                return {\n                    'image': image,\n                    'target': row.city_id,\n                    'filename': row.filename\n                }\n        except Exception as e:\n            print(f\"Error in __getitem__ at index {index}: {str(e)}\")\n            raise e\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:17.986842Z","iopub.execute_input":"2024-12-16T02:08:17.987135Z","iopub.status.idle":"2024-12-16T02:08:18.001588Z","shell.execute_reply.started":"2024-12-16T02:08:17.987109Z","shell.execute_reply":"2024-12-16T02:08:18.000724Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import timm\nfrom timm import create_model\n\nclass EfficientNetB6Head(nn.Module):\n    def __init__(self, num_classes, dropout_rate=0.5):\n        super().__init__()\n        self.encoder = create_model(\n            'tf_efficientnet_b6_ns',\n            pretrained=True,\n            num_classes=0\n        )\n        \n        # Freeze some early layers\n        for name, param in list(self.encoder.named_parameters())[:100]:\n            param.requires_grad = False\n            \n        n_features = self.encoder.num_features\n        \n        # More gradual reduction in dimensions\n        # Added LayerNorm for better regularization\n        self.head = nn.Sequential(\n            nn.Linear(n_features, 1536),\n            nn.LayerNorm(1536),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(1536, 768),\n            nn.LayerNorm(768),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(768, 384),\n            nn.LayerNorm(384),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(384, num_classes)\n        )\n        \n    def forward(self, x):\n        features = self.encoder(x)\n        return self.head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:18.002630Z","iopub.execute_input":"2024-12-16T02:08:18.002896Z","iopub.status.idle":"2024-12-16T02:08:18.019120Z","shell.execute_reply.started":"2024-12-16T02:08:18.002852Z","shell.execute_reply":"2024-12-16T02:08:18.018298Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def prepare_data(train_df):\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n    train_df['fold'] = -1\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['city'])):\n        train_df.loc[val_idx, 'fold'] = fold\n    \n    return train_df\n\ndef load_data(train_df, test_df, train_dir, test_dir, fold=0, mean=None, std=None):\n    print(\"Preparing data loaders...\")\n    \n    label_encoder = LabelEncoder()\n    train_df['city_id'] = label_encoder.fit_transform(train_df['city'])\n    num_classes = len(label_encoder.classes_)\n    \n    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n    valid_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n    \n    train_dataset = ImageDataset(train_data, train_dir, mode='train', mean=mean, std=std)\n    valid_dataset = ImageDataset(valid_data, train_dir, mode='valid', mean=mean, std=std)\n    test_dataset = ImageDataset(test_df, test_dir, mode='test', mean=mean, std=std)\n\n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Validation dataset size: {len(valid_dataset)}\")\n    print(f\"Test dataset size: {len(test_dataset)}\")\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        drop_last=True\n    )\n    \n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    return train_loader, valid_loader, test_loader, label_encoder, num_classes\n\ndef calculate_macro_f1(preds, targets, num_classes):\n\n    # Tensor'ları numpy array'e çevir\n    if torch.is_tensor(preds):\n        preds = preds.cpu().numpy()\n    if torch.is_tensor(targets):\n        targets = targets.cpu().numpy()\n    \n    # Her şehir için F1 skorunu hesapla\n    city_f1_scores = []\n    \n    for city_idx in range(num_classes):\n        # True Positives: Doğru tahmin edilen şehir sayısı\n        tp = np.sum((preds == city_idx) & (targets == city_idx))\n        \n        # False Positives: Yanlış şehir olarak tahmin edilenler\n        fp = np.sum((preds == city_idx) & (targets != city_idx))\n        \n        # False Negatives: Kaçırılan şehir tahminleri\n        fn = np.sum((preds != city_idx) & (targets == city_idx))\n        \n        # Precision hesapla\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        \n        # Recall hesapla\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        \n        # F1 skoru hesapla\n        if precision + recall > 0:\n            f1 = 2 * (precision * recall) / (precision + recall)\n        else:\n            f1 = 0.0\n        \n        city_f1_scores.append(f1)\n    \n    # Macro F1: Tüm şehirlerin F1 skorlarının ortalaması\n    macro_f1 = np.mean(city_f1_scores)\n    \n    return float(macro_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:18.020250Z","iopub.execute_input":"2024-12-16T02:08:18.020495Z","iopub.status.idle":"2024-12-16T02:08:18.039634Z","shell.execute_reply.started":"2024-12-16T02:08:18.020470Z","shell.execute_reply":"2024-12-16T02:08:18.038794Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_step(train_loader, model, criterion, optimizer, epoch, scaler):\n    model.train()\n    running_loss = 0.0\n    running_f1 = 0.0\n    steps = 0\n    \n    optimizer.zero_grad(set_to_none=True)\n    \n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for i, data in pbar:\n        images = data['image'].cuda(non_blocking=True)\n        targets = data['target'].cuda(non_blocking=True)\n        \n        with torch.amp.autocast(device_type='cuda'):\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            loss = loss / GRADIENT_ACCUMULATION_STEPS\n        \n        # Mixed precision backward pass\n        scaler.scale(loss).backward()\n        \n        # Gradient accumulation\n        if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n        \n        # Calculate metrics\n        with torch.no_grad():\n            _, preds = torch.max(outputs, 1)\n            f1 = calculate_macro_f1(preds, targets, outputs.size(1))\n        \n        # Update metrics\n        running_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n        running_f1 += f1\n        steps += 1\n        \n        # Update progress bar\n        pbar.set_description(\n            f'Epoch {epoch} - Loss: {running_loss/steps:.4f}, F1: {running_f1/steps:.4f}'\n        )\n        \n        # Clear memory\n        del images, outputs, loss\n        torch.cuda.empty_cache()\n    \n    return running_f1 / steps\n\ndef train_model(train_loader, valid_loader, model, criterion, optimizer, scheduler, scaler):\n    \"\"\"Training loop with early stopping\"\"\"\n    patience = PATIENCE\n    counter = 0\n    best_valid_f1 = 0.0\n    epoch = 0\n    \n    while True:\n        epoch += 1\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        # Train\n        train_f1 = train_step(train_loader, model, criterion, optimizer, epoch, scaler)\n        \n        # Validate\n        valid_loss, valid_f1 = validate(valid_loader, model, criterion)\n        \n        # Scheduler step\n        scheduler.step()\n        \n        print(f\"Epoch {epoch} - Train F1: {train_f1:.4f}, Valid F1: {valid_f1:.4f}\")\n        \n        if valid_f1 > best_valid_f1:\n            best_valid_f1 = valid_f1\n            counter = 0\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),  # scheduler'ı da kaydedelim\n                'epoch': epoch,\n                'best_f1': best_valid_f1,\n            }, 'best_model.pth')\n            print(f\"Saved best model with F1: {best_valid_f1:.4f}\")\n        else:\n            counter += 1\n            if counter >= patience:\n                print(f\"Early stopping at epoch {epoch}\")\n                break\n    \n    return best_valid_f1  \n\n@torch.no_grad()\ndef validate(valid_loader, model, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_f1 = 0.0\n    steps = 0\n    \n    for data in tqdm(valid_loader, desc='Validating'):\n        images = data['image'].cuda(non_blocking=True)\n        targets = data['target'].cuda(non_blocking=True)\n        \n        # Updated autocast syntax\n        with torch.amp.autocast(device_type='cuda'):\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n        \n        _, preds = torch.max(outputs, 1)\n        f1 = calculate_macro_f1(preds, targets, outputs.size(1))\n        \n        running_loss += loss.item()\n        running_f1 += f1\n        steps += 1\n        \n        del images, outputs, loss\n        torch.cuda.empty_cache()\n    \n    return running_loss / steps, running_f1 / steps\n\n@torch.no_grad()\ndef predict(test_loader, model, label_encoder):\n    model.eval()\n    predictions = []\n    filenames = []\n    \n    for data in tqdm(test_loader):\n        images = data['image'].cuda(non_blocking=True)\n        \n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n        \n        predictions.extend(label_encoder.inverse_transform(preds.cpu().numpy()))\n        filenames.extend(data['filename'])\n        \n        del images, outputs, preds\n        torch.cuda.empty_cache()\n    \n    return filenames, predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:18.040763Z","iopub.execute_input":"2024-12-16T02:08:18.041036Z","iopub.status.idle":"2024-12-16T02:08:18.056569Z","shell.execute_reply.started":"2024-12-16T02:08:18.041008Z","shell.execute_reply":"2024-12-16T02:08:18.055795Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def generate_submission(test_loader, model, label_encoder):\n    filenames, predictions = predict(test_loader, model, label_encoder)\n    \n    submission = pd.DataFrame({\n        'filename': filenames,\n        'city': predictions\n    })\n    \n    submission.to_csv('submission.csv', index=False)\n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:08:18.058459Z","iopub.execute_input":"2024-12-16T02:08:18.058772Z","iopub.status.idle":"2024-12-16T02:08:18.070467Z","shell.execute_reply.started":"2024-12-16T02:08:18.058745Z","shell.execute_reply":"2024-12-16T02:08:18.069706Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Kaggle paths\n    KAGGLE_INPUT = '/kaggle/input/datathon-ai-qualification-round'\n    \n    # Load data\n    train = pd.read_csv(f'{KAGGLE_INPUT}/train_data.csv')\n    test = pd.read_csv(f'{KAGGLE_INPUT}/test.csv')\n    \n    # Set correct image directories\n    train_dir = f'{KAGGLE_INPUT}/train/train'\n    test_dir = f'{KAGGLE_INPUT}/test/test'\n    \n    # Print dataset info\n    print(\"Dataset Information:\")\n    print(f\"Training samples: {len(train)}\")\n    print(f\"Test samples: {len(test)}\")\n    print(\"\\nSample training data:\")\n    print(train.head())\n    print(\"\\nSample test data:\")\n    print(test.head())\n    \n    # Verify paths exist\n    for path in [train_dir, test_dir]:\n        if not os.path.exists(path):\n            raise ValueError(f\"Path does not exist: {path}\")\n    \n    # First prepare folds - MOVED BEFORE stats calculation\n    train = prepare_data(train)\n    \n    # Calculate dataset statistics ONLY on training fold\n    # Get the data for the training fold (excluding validation data)\n    training_fold = 0  # Assuming we're using fold 0 for validation\n    train_fold_data = train[train['fold'] != training_fold].reset_index(drop=True)\n    \n    print(f\"Calculating statistics using {len(train_fold_data)} training samples...\")\n    train_mean, train_std = calculate_dataset_stats(train_fold_data, train_dir)\n    \n    # Enable memory optimizations\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    try:\n        # Set device\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {device}\")\n        \n        # Initialize mixed precision training\n        scaler = torch.amp.GradScaler(device='cuda')\n        \n        # Train on fold 0\n        train_loader, valid_loader, test_loader, label_encoder, num_classes = load_data(\n            train, test, train_dir, test_dir, fold=training_fold,\n            mean=train_mean.tolist(),\n            std=train_std.tolist()\n        )\n        \n        # Initialize model\n        model = EfficientNetB6Head(num_classes=num_classes)\n        model = model.to(device)\n        \n        # Optimizer and criterion\n        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n        optimizer = optim.AdamW(\n        model.parameters(),\n        lr=5e-4,  # Daha düşük learning rate\n        weight_decay=0.05,  # Daha yüksek weight decay\n        betas=(0.9, 0.999)\n    )\n    \n        # Scheduler\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=30,  # maximum epoch sayısı\n        eta_min=1e-6  # minimum learning rate\n    )\n        \n        print(\"Starting training...\")\n        # Train model with early stopping\n        best_valid_f1 = train_model(\n            train_loader, valid_loader, model, \n            criterion, optimizer, scheduler, scaler \n        )\n        \n        print(\"Loading best model for submission...\")\n        # Load best model and generate submission\n        checkpoint = torch.load('best_model.pth')\n        model.load_state_dict(checkpoint['model_state_dict'])\n        \n        print(\"Generating submission file...\")\n        submission = generate_submission(test_loader, model, label_encoder)\n        \n        # Save submission\n        submission_path = '/kaggle/working/submission.csv'\n        submission.to_csv(submission_path, index=False)\n        \n        print(f\"Training completed. Best validation F1: {best_valid_f1:.4f}\")\n        print(f\"Submission saved to: {submission_path}\")\n        \n        # Verify submission format\n        print(\"\\nVerifying submission format...\")\n        if set(submission.columns) != {'filename', 'city'}:\n            print(\"Warning: Submission columns do not match required format!\")\n        if not all(submission['city'].isin(['Istanbul', 'Ankara', 'Izmir'])):\n            print(\"Warning: Submission contains invalid city names!\")\n        \n    except Exception as e:\n        print(f\"An error occurred during training: {str(e)}\")\n        raise\n    \n    finally:\n        # Clean up\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:10:20.528275Z","iopub.execute_input":"2024-12-16T02:10:20.528641Z","iopub.status.idle":"2024-12-16T04:03:44.604564Z","shell.execute_reply.started":"2024-12-16T02:10:20.528611Z","shell.execute_reply":"2024-12-16T04:03:44.603584Z"}},"outputs":[{"name":"stdout","text":"Dataset Information:\nTraining samples: 7000\nTest samples: 2000\n\nSample training data:\n          filename      city\n0  image_10000.jpg  Istanbul\n1  image_10001.jpg  Istanbul\n2  image_10002.jpg    Ankara\n3  image_10003.jpg     Izmir\n4  image_10004.jpg    Ankara\n\nSample test data:\n          filename  city\n0  image_17000.jpg   NaN\n1  image_17001.jpg   NaN\n2  image_17002.jpg   NaN\n3  image_17003.jpg   NaN\n4  image_17004.jpg   NaN\nCalculating statistics using 5600 training samples...\nCalculating dataset mean and std...\n","output_type":"stream"},{"name":"stderr","text":"Calculating dataset statistics: 100%|██████████| 175/175 [00:45<00:00,  3.85it/s]\n/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b6_ns to current tf_efficientnet_b6.ns_jft_in1k.\n  model = create_fn(\n","output_type":"stream"},{"name":"stdout","text":"Dataset mean: tensor([0.5070, 0.5327, 0.5378])\nDataset std: tensor([0.2318, 0.2396, 0.2917])\nUsing device: cuda\nPreparing data loaders...\nTrain dataset size: 5600\nValidation dataset size: 1400\nTest dataset size: 2000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/173M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb43494faa1d4b56a39390b8f1234732"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 - Loss: 0.9319, F1: 0.5832: 100%|██████████| 350/350 [03:11<00:00,  1.83it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Train F1: 0.5832, Valid F1: 0.7315\nSaved best model with F1: 0.7315\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 - Loss: 0.7712, F1: 0.7154: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Train F1: 0.7154, Valid F1: 0.7552\nSaved best model with F1: 0.7552\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 - Loss: 0.6881, F1: 0.7727: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Train F1: 0.7727, Valid F1: 0.8044\nSaved best model with F1: 0.8044\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 - Loss: 0.6474, F1: 0.7949: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Train F1: 0.7949, Valid F1: 0.8142\nSaved best model with F1: 0.8142\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 - Loss: 0.6181, F1: 0.8128: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Train F1: 0.8128, Valid F1: 0.8457\nSaved best model with F1: 0.8457\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 - Loss: 0.5788, F1: 0.8317: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Train F1: 0.8317, Valid F1: 0.8447\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 - Loss: 0.5375, F1: 0.8617: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Train F1: 0.8617, Valid F1: 0.8428\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 - Loss: 0.5107, F1: 0.8753: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Train F1: 0.8753, Valid F1: 0.8692\nSaved best model with F1: 0.8692\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 - Loss: 0.4932, F1: 0.8843: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Train F1: 0.8843, Valid F1: 0.8762\nSaved best model with F1: 0.8762\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 - Loss: 0.4689, F1: 0.9005: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Train F1: 0.9005, Valid F1: 0.8820\nSaved best model with F1: 0.8820\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 - Loss: 0.4633, F1: 0.9009: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 - Train F1: 0.9009, Valid F1: 0.8681\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 - Loss: 0.4410, F1: 0.9146: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 - Train F1: 0.9146, Valid F1: 0.8662\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 - Loss: 0.4206, F1: 0.9284: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 - Train F1: 0.9284, Valid F1: 0.8881\nSaved best model with F1: 0.8881\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 - Loss: 0.3999, F1: 0.9380: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 - Train F1: 0.9380, Valid F1: 0.8948\nSaved best model with F1: 0.8948\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 - Loss: 0.3967, F1: 0.9407: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 - Train F1: 0.9407, Valid F1: 0.8809\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16 - Loss: 0.3678, F1: 0.9579: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 - Train F1: 0.9579, Valid F1: 0.8992\nSaved best model with F1: 0.8992\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17 - Loss: 0.3572, F1: 0.9622: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 - Train F1: 0.9622, Valid F1: 0.8909\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18 - Loss: 0.3504, F1: 0.9682: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 - Train F1: 0.9682, Valid F1: 0.8757\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19 - Loss: 0.3391, F1: 0.9733: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 - Train F1: 0.9733, Valid F1: 0.8970\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20 - Loss: 0.3375, F1: 0.9740: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 - Train F1: 0.9740, Valid F1: 0.8986\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21 - Loss: 0.3267, F1: 0.9801: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 - Train F1: 0.9801, Valid F1: 0.9022\nSaved best model with F1: 0.9022\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22 - Loss: 0.3250, F1: 0.9809: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 - Train F1: 0.9809, Valid F1: 0.9066\nSaved best model with F1: 0.9066\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23 - Loss: 0.3268, F1: 0.9801: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 - Train F1: 0.9801, Valid F1: 0.9014\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24 - Loss: 0.3160, F1: 0.9849: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 - Train F1: 0.9849, Valid F1: 0.9053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25 - Loss: 0.3114, F1: 0.9894: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 - Train F1: 0.9894, Valid F1: 0.9089\nSaved best model with F1: 0.9089\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26 - Loss: 0.3116, F1: 0.9888: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 - Train F1: 0.9888, Valid F1: 0.9045\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27 - Loss: 0.3026, F1: 0.9953: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 - Train F1: 0.9953, Valid F1: 0.9131\nSaved best model with F1: 0.9131\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28 - Loss: 0.3061, F1: 0.9917: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 - Train F1: 0.9917, Valid F1: 0.9192\nSaved best model with F1: 0.9192\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29 - Loss: 0.3053, F1: 0.9934: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 - Train F1: 0.9934, Valid F1: 0.9134\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30 - Loss: 0.3048, F1: 0.9949: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 - Train F1: 0.9949, Valid F1: 0.9158\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31 - Loss: 0.3037, F1: 0.9937: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31 - Train F1: 0.9937, Valid F1: 0.9189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32 - Loss: 0.3079, F1: 0.9900: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32 - Train F1: 0.9900, Valid F1: 0.9147\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33 - Loss: 0.3036, F1: 0.9938: 100%|██████████| 350/350 [03:10<00:00,  1.84it/s]\nValidating: 100%|██████████| 88/88 [00:12<00:00,  6.95it/s]\n/tmp/ipykernel_23/2063660693.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('best_model.pth')\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33 - Train F1: 0.9938, Valid F1: 0.9191\nEarly stopping at epoch 33\nLoading best model for submission...\nGenerating submission file...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/125 [00:00<?, ?it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  1%|          | 1/125 [00:00<01:10,  1.76it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  2%|▏         | 2/125 [00:00<00:39,  3.12it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  2%|▏         | 3/125 [00:00<00:30,  4.05it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  3%|▎         | 4/125 [00:01<00:25,  4.83it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  4%|▍         | 5/125 [00:01<00:23,  5.18it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  5%|▍         | 6/125 [00:01<00:21,  5.65it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  6%|▌         | 7/125 [00:01<00:20,  5.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  6%|▋         | 8/125 [00:01<00:18,  6.16it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  7%|▋         | 9/125 [00:01<00:18,  6.21it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  8%|▊         | 10/125 [00:01<00:17,  6.40it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n  9%|▉         | 11/125 [00:02<00:17,  6.55it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 10%|▉         | 12/125 [00:02<00:16,  6.65it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 10%|█         | 13/125 [00:02<00:16,  6.72it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 11%|█         | 14/125 [00:02<00:16,  6.78it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 12%|█▏        | 15/125 [00:02<00:16,  6.67it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 13%|█▎        | 16/125 [00:02<00:16,  6.74it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 14%|█▎        | 17/125 [00:02<00:16,  6.71it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 14%|█▍        | 18/125 [00:03<00:15,  6.73it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 15%|█▌        | 19/125 [00:03<00:15,  6.78it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 16%|█▌        | 20/125 [00:03<00:15,  6.81it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 17%|█▋        | 21/125 [00:03<00:15,  6.84it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 18%|█▊        | 22/125 [00:03<00:15,  6.86it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 18%|█▊        | 23/125 [00:03<00:14,  6.86it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 19%|█▉        | 24/125 [00:03<00:14,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 20%|██        | 25/125 [00:04<00:14,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 21%|██        | 26/125 [00:04<00:14,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 22%|██▏       | 27/125 [00:04<00:14,  6.80it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 22%|██▏       | 28/125 [00:04<00:14,  6.83it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 23%|██▎       | 29/125 [00:04<00:14,  6.68it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 24%|██▍       | 30/125 [00:04<00:14,  6.74it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 25%|██▍       | 31/125 [00:05<00:13,  6.79it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 26%|██▌       | 32/125 [00:05<00:13,  6.82it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 26%|██▋       | 33/125 [00:05<00:13,  6.83it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 27%|██▋       | 34/125 [00:05<00:13,  6.86it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 28%|██▊       | 35/125 [00:05<00:13,  6.87it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 29%|██▉       | 36/125 [00:05<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 30%|██▉       | 37/125 [00:05<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 30%|███       | 38/125 [00:06<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 31%|███       | 39/125 [00:06<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 32%|███▏      | 40/125 [00:06<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 33%|███▎      | 41/125 [00:06<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 34%|███▎      | 42/125 [00:06<00:12,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 34%|███▍      | 43/125 [00:06<00:11,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 35%|███▌      | 44/125 [00:06<00:11,  6.90it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 36%|███▌      | 45/125 [00:07<00:11,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 37%|███▋      | 46/125 [00:07<00:11,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 38%|███▊      | 47/125 [00:07<00:11,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 38%|███▊      | 48/125 [00:07<00:11,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 39%|███▉      | 49/125 [00:07<00:11,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 40%|████      | 50/125 [00:07<00:10,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 41%|████      | 51/125 [00:07<00:10,  6.82it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 42%|████▏     | 52/125 [00:08<00:10,  6.84it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 42%|████▏     | 53/125 [00:08<00:10,  6.66it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 43%|████▎     | 54/125 [00:08<00:10,  6.72it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 44%|████▍     | 55/125 [00:08<00:10,  6.62it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 45%|████▍     | 56/125 [00:08<00:10,  6.69it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 46%|████▌     | 57/125 [00:08<00:10,  6.70it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 46%|████▋     | 58/125 [00:08<00:09,  6.75it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 47%|████▋     | 59/125 [00:09<00:09,  6.80it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 48%|████▊     | 60/125 [00:09<00:09,  6.83it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 49%|████▉     | 61/125 [00:09<00:09,  6.85it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 50%|████▉     | 62/125 [00:09<00:09,  6.87it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 50%|█████     | 63/125 [00:09<00:09,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 51%|█████     | 64/125 [00:09<00:08,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 52%|█████▏    | 65/125 [00:09<00:08,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 53%|█████▎    | 66/125 [00:10<00:08,  6.86it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 54%|█████▎    | 67/125 [00:10<00:08,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 54%|█████▍    | 68/125 [00:10<00:08,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 55%|█████▌    | 69/125 [00:10<00:08,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 56%|█████▌    | 70/125 [00:10<00:08,  6.73it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 57%|█████▋    | 71/125 [00:10<00:07,  6.78it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 58%|█████▊    | 72/125 [00:11<00:07,  6.82it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 58%|█████▊    | 73/125 [00:11<00:07,  6.85it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 59%|█████▉    | 74/125 [00:11<00:07,  6.86it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 60%|██████    | 75/125 [00:11<00:07,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 61%|██████    | 76/125 [00:11<00:07,  6.70it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 62%|██████▏   | 77/125 [00:11<00:07,  6.76it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 62%|██████▏   | 78/125 [00:11<00:06,  6.80it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 63%|██████▎   | 79/125 [00:12<00:06,  6.83it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 64%|██████▍   | 80/125 [00:12<00:06,  6.82it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 65%|██████▍   | 81/125 [00:12<00:06,  6.85it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 66%|██████▌   | 82/125 [00:12<00:06,  6.68it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 66%|██████▋   | 83/125 [00:12<00:06,  6.68it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 67%|██████▋   | 84/125 [00:12<00:06,  6.74it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 68%|██████▊   | 85/125 [00:12<00:06,  6.57it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 69%|██████▉   | 86/125 [00:13<00:05,  6.67it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 70%|██████▉   | 87/125 [00:13<00:05,  6.60it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 70%|███████   | 88/125 [00:13<00:05,  6.68it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 71%|███████   | 89/125 [00:13<00:05,  6.57it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 72%|███████▏  | 90/125 [00:13<00:05,  6.66it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 73%|███████▎  | 91/125 [00:13<00:05,  6.48it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 74%|███████▎  | 92/125 [00:14<00:05,  6.60it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 74%|███████▍  | 93/125 [00:14<00:05,  6.03it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 75%|███████▌  | 94/125 [00:14<00:04,  6.27it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 76%|███████▌  | 95/125 [00:14<00:04,  6.04it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 77%|███████▋  | 96/125 [00:14<00:04,  6.21it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 78%|███████▊  | 97/125 [00:14<00:04,  6.17it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 78%|███████▊  | 98/125 [00:14<00:04,  6.38it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 79%|███████▉  | 99/125 [00:15<00:04,  6.05it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 80%|████████  | 100/125 [00:15<00:04,  6.12it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 81%|████████  | 101/125 [00:15<00:03,  6.19it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 82%|████████▏ | 102/125 [00:15<00:03,  6.32it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 82%|████████▏ | 103/125 [00:15<00:03,  6.32it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 83%|████████▎ | 104/125 [00:15<00:03,  6.48it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 84%|████████▍ | 105/125 [00:16<00:03,  6.46it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 85%|████████▍ | 106/125 [00:16<00:02,  6.58it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 86%|████████▌ | 107/125 [00:16<00:02,  6.40it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 86%|████████▋ | 108/125 [00:16<00:02,  6.55it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 87%|████████▋ | 109/125 [00:16<00:02,  6.50it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 88%|████████▊ | 110/125 [00:16<00:02,  6.61it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 89%|████████▉ | 111/125 [00:17<00:02,  6.68it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 90%|████████▉ | 112/125 [00:17<00:01,  6.75it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 90%|█████████ | 113/125 [00:17<00:01,  6.79it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 91%|█████████ | 114/125 [00:17<00:01,  6.83it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 92%|█████████▏| 115/125 [00:17<00:01,  6.85it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 93%|█████████▎| 116/125 [00:17<00:01,  6.86it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 94%|█████████▎| 117/125 [00:17<00:01,  6.87it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 94%|█████████▍| 118/125 [00:18<00:01,  6.88it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 95%|█████████▌| 119/125 [00:18<00:00,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 96%|█████████▌| 120/125 [00:18<00:00,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 97%|█████████▋| 121/125 [00:18<00:00,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 98%|█████████▊| 122/125 [00:18<00:00,  6.89it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 98%|█████████▊| 123/125 [00:18<00:00,  6.90it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n 99%|█████████▉| 124/125 [00:18<00:00,  6.91it/s]/tmp/ipykernel_23/152104679.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 125/125 [00:19<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training completed. Best validation F1: 0.9192\nSubmission saved to: /kaggle/working/submission.csv\n\nVerifying submission format...\n","output_type":"stream"}],"execution_count":13}]}